
# 208POMC â€” Waveform-Based POMC Classification Pipeline

This repository contains a fully modular machine learning pipeline for training, evaluating, reporting, and comparing predictive models for **Postoperative Major Complications (POMC)** using **ABP** and **PPG** waveformâ€“derived features.

The pipeline supports:

- Modular preprocessing  
- Configâ€‘driven model training  
- Evaluation (ROC/PR/AP/Brier/confusion)  
- Sexâ€‘stratified subgroup performance  
- Markdown report generation  
- Automatic plot generation  
- Multiâ€‘run comparison (DeLong + McNemar)  
- Timestamped, reproducible folder outputs  
- Centralized Python logging (perâ€‘action log files)

All pipeline components live in `pipeline/` and are orchestrated through:

```
python main.py
```

## Models implemented
For now, only the Gradient Boost Classifier (GBC) has been implemented.

---

## ğŸ” Quick Start

```bash
# 1. Preprocess both datasets
python main.py --action preprocess --datasets abp,ppg

# 2. Train model (uses latest preprocess)
python main.py --action train --datasets abp

# 3. Evaluate trained model
python main.py --action evaluate --datasets abp

# 4. Generate report + plots
python main.py --action report --datasets abp

# 5. Compare two runs
python main.py --action compare --datasets abp   --runs run_20250117_210055,run_20250118_093212
```

---

# ğŸš€ Full Usage Guide

## General CLI pattern
```
python main.py --action <ACTION> --datasets <abp,ppg> [other options]
```

If omitted, `--datasets` defaults to **abp**.

---

# 1ï¸âƒ£ PREPROCESSING

Creates a new timestamped `preprocess_<timestamp>` folder:

```
python main.py --action preprocess --datasets abp,ppg
```

Artifacts saved under:
```
outputs/<dataset>/gbc/preprocess_<timestamp>/
    splits.json
    feature_names.json
    imputer.joblib
    metadata.json
```

---

# 2ï¸âƒ£ TRAINING

Train a model using an existing preprocess folder.

Use latest automatically:
```
python main.py --action train --datasets abp
```

Explicit preprocess folder:
```
python main.py --action train --datasets ppg   --preprocess-folder preprocess_20250118_142233
```

Artifacts saved:
```
outputs/<dataset>/gbc/run_<timestamp>/
    model.joblib
    preds_test.npy
    y_test.npy
    training_metadata.json
```

---

# 3ï¸âƒ£ EVALUATION

Runs all evaluation metrics and saves `evaluation.json`.

Evaluate latest run:
```
python main.py --action evaluate --datasets abp
```

Specify a run folder:
```
python main.py --action evaluate   --datasets ppg   --run-folder run_20250118_145012
```

Evaluation output:
```
outputs/<dataset>/gbc/run_<timestamp>/evaluation.json
```

---

# 4ï¸âƒ£ REPORT GENERATION

Creates markdown summary of all metrics + plots:

```
python main.py --action report --datasets abp
```

Outputs:
```
outputs/<dataset>/gbc/run_<timestamp>/report/report.md
outputs/<dataset>/gbc/run_<timestamp>/plots/*.png
```

Plots include:
- ROC curve  
- PR curve  
- Calibration curve  
- Probability histogram  
- Sexâ€‘stratified ROC/PR  
- Feature importances  
- SHAP (if enabled)

---

# 5ï¸âƒ£ MODEL COMPARISON

Compare two or more runs using statistical tests.

```
python main.py --action compare --datasets abp   --runs run_20250117_210055,run_20250118_093212
```

Outputs:
```
outputs/compare/<dataset>/compare_<timestamp>.md
```
Includes:
- AUROC difference (DeLong)
- Classification disagreement (McNemar)
- Per-run metrics summary

---

# ğŸ“ Project Structure

```
208pomc/
â”‚
â”œâ”€â”€ main.py                 # CLI entrypoint
â”œâ”€â”€ run_config.yaml         # paths, seeds, preprocessing settings
â”œâ”€â”€ model_config.yaml       # model hyperparameter grids
â”‚
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ loader.py           # loading YAML, artifacts, datasets
â”‚   â”œâ”€â”€ logger.py           # log configuration
â”‚   â”œâ”€â”€ preprocess.py       # preprocessing + stratified splits
â”‚   â”œâ”€â”€ training.py         # training + saving metadata
â”‚   â”œâ”€â”€ evaluator.py        # metric computation
â”‚   â”œâ”€â”€ plotting.py         # all plots (ROC, PR, calibration, SHAP)
â”‚   â”œâ”€â”€ reporting.py        # markdown model report generator
â”‚   â””â”€â”€ compare.py          # multi-run statistical comparison
â”‚
â”œâ”€â”€ inputs/
â”œâ”€â”€ outputs/
â””â”€â”€ logs/
```

Output hierarchy:
```
outputs/<dataset>/gbc/
    preprocess_<timestamp>/
    run_<timestamp>/
```

---

# ğŸ§ª Endâ€‘toâ€‘End Example

```
python main.py --action preprocess --datasets abp,ppg
python main.py --action train --datasets abp
python main.py --action evaluate --datasets abp
python main.py --action report --datasets abp
python main.py --action compare --datasets abp   --runs run_1,run_2
```

---

# ğŸ“Š Visual Diagram

```
     raw CSVs
         â”‚
         â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     PREPROCESSING
  stratified splits
  imputation
  metadata/features
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â”‚
         â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRAINING
  hyperparameter search
  final model fit
  predictions saved
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â”‚
         â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        EVALUATE
  ROC/PR/AP/Brier
  confusion metrics
  sexâ€‘stratified metrics
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â”‚
         â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   REPORT + PLOTTING
  markdown report
  ROC/PR/calibration
  SHAP + feature import
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â”‚
         â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        COMPARE
  statistical tests
  comparison markdown
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

---

# ğŸ“ Logging

All pipeline steps automatically create log files in `logs/`.

Log naming pattern:
```
logs/<timestamp>_<action>.log
```

Example:
```
logs/20250118_150233_train.log
```

Each log captures:
- Action start + end times  
- Perâ€‘dataset step timing  
- Key configuration values  
- Errors / warnings  
- Console output redirected  
- Critical pipeline decisions (autodetected run folders, etc.)

---

ğŸ“ Configuration Files

The pipeline uses two YAML files to control behavior without modifying code:

```run_config.yaml```
Defines global pipeline settings, including:
- locations of input CSV files
- where outputs are written
- timestamp format
- preprocessing options (e.g., seed, imputer strategy)
- dataset-specific file names

This file governs how data flows through the pipeline and how output folders are created.

```model_config.yaml```
Defines model-specific settings, including:
- which model(s) are available
- hyperparameter grids used during training
- any model-level default parameters

This file allows you to adjust or add models without touching the training code.
